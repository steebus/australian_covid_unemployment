---
title: "Predicting Unemployment"
author: "Steve Jones"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting Unemployment Rates in Australia: A Machine Learning Approach

## Project Overview

The goal of this project was to develop models to predict unemployment rates in Australia by analyzing various economic indicators. This project covered data from 1999 to 2020, including the initial impact of the COVID-19 pandemic.

### 1. The Challenge

The goal was to predict Australia's unemployment rate using economic data. Achieving this could have several positive impacts across different areas:
- Business planning for hiring strategies
- Governments policy development
- Greater understanding of economic trends

```{r message=FALSE, warning=FALSE}
# load packages----
library(readxl)
library(patchwork)
library(readr)
library(tidyverse)
library(ggplot2)
library(GGally)
```

### 2. The Data

We're working with quarterly data from the Australian Bureau of Statistics from the year 1981 to 2020. These includes:
- Consumer Price Index (CPI)
- Population estimates
- Job vacancy numbers
- Government spending figures
- Trade information
- Overall economic growth rates

```{r message=FALSE, warning=FALSE}
# Load the Data----
data <- read_excel("H:/My Drive/_PROJECTS/_Portfolio/Capstone Projects/Machine Learning - A3 (Unemployment Prediction)/AUS_Data.xlsx",col_names = TRUE,skip = 1,na = "NA")
```

After loading the initial data set we first tidy the column names and check for incomplete observations.

```{r}
# tidy data----
names(data) <- c("period",
                 "unemploymentRate",
                 "grossDomesticProduct",
                 "generalGovFCE",
                 "AllSectorsFCE",
                 "termsOfTradeIndex",
                 "CPI",
                 "jobVacancies",
                 "estimatedResidentPopulation")

# create a table for the rows and column names of missing values
data %>%
  pivot_longer(cols = -1,
               names_to = "column_name", 
               values_to = "value") %>%
  filter(is.na(value)) %>%
  select(1, column_name) %>% 
  arrange(period)
```

There are 10 observations with missing values. The first five are all for 'jobVacancies' between late 2008 and late 2009. When looking into this gap in the data, a statement from the ABS was found stating:

> *"As a result of JVS* (Job Vacancies Survey) *being suspended, there will be a gap in all series: original, seasonally adjusted and trend, for five quarters between August 2008 and August 2009 inclusive. The ABS cannot produce reliable estimates by collecting this missing data retrospectively, and has not been able to fill the gap using other data sources and modelling techniques."*

As this data is unavailable, we will impute the missing values. First, let's look at the data:
```{r}
print(paste("Median:",median(data$jobVacancies,na.rm=TRUE)))
print(paste("Mean:",mean(data$jobVacancies,na.rm=TRUE)))
```

```{r warning=FALSE}
job_vacancy_line_plot <- ggplot(data,aes(x=period, y = jobVacancies))+
    geom_line()+
    labs(title = "Job Vacancy over time",y="Job Vacancy Rate")

job_vacancy_histogram <- ggplot(data,aes(x=jobVacancies))+
  geom_histogram(binwidth=10)+
  labs(title="Job Vacancy distribution")

job_vacancy_histogram + job_vacancy_line_plot # use the patchwork package
```

As seen in the line plot, the gap hides a drop of roughly 40 points. Imputing with the mean or median of the column is not feasible as it would create a short sharp drop of 75 points over that period. 

A simple alternative would be to linearly plot the gap between the two existing points (row 109 and row 115).
```{r}
data$jobVacancies[110:114] <- seq(data$jobVacancies[109],data$jobVacancies[115],length.out=7)[2:6] # fill the missing data with a linear sequence between existing vales 

job_vacancy_line_plot <- ggplot(data,aes(x=period, y = jobVacancies))+
    geom_line()+
    labs(title = "Job Vacancy over time",y="Job Vacancy Rate")

job_vacancy_histogram <- ggplot(data,aes(x=jobVacancies))+
  geom_histogram(binwidth=10)+
  labs(title="Job Vacancy distribution")

job_vacancy_histogram + job_vacancy_line_plot # use the patchwork package
```

```{r}
print(paste("Median:",median(data$jobVacancies,na.rm=TRUE)))
print(paste("Mean:",mean(data$jobVacancies,na.rm=TRUE)))
```

Our linear fill has left the distribution mostly unaltered. The median and mean have increased very slightly.

The remaining incomplete observations are missing population values between early 2019 and late 2020. This data can be sourced from alternative reports by ABS. 
```{r message=FALSE, warning=FALSE}
## impute missing values with alternate source----
alt.data <- read.csv("ABS_ERP_Q_1.0.0_1.3.TOT..Q.csv")[663:667,7:8]
print(alt.data)
data$estimatedResidentPopulation[154:158] <- alt.data$OBS_VALUE/100 # divided by 100 to account for the rounding in the original dataset.
```

We've now accounted for all NA values, lets have a look at our target variable, the unemployment rate.

```{r}
## fig 1.1 unemployment rate----
ggplot(data,aes(x=period, y = unemploymentRate))+
    geom_line()+
    geom_hline(yintercept = max(data$unemploymentRate),
               linetype="dashed",
               colour="red",
               linewidth=1)+
  ylab("Unemployment Rate (%)")
```

Over the last 40 years we can see a large variation in the unemployment rate. Peaking at just over 11% in 1993 and at its lowest of approximately 4% in 2008. Of particular interest is the spike at the end of the period due to the COVID-19 pandemic.  

The observations from 1980 through to 2012 account for roughly 80% of the data set and will be used as training data. The remaining 20% will be withheld from the training process and used to test the models' performance.

Let's look at the predictor variables.
```{r}
## fig 1.2 data set----
(plot.all <- data |> 
    gather(-c(period,unemploymentRate), key = "variable", value = "value") |> 
    ggplot(aes(x = period, y = value)) +
    geom_line() +
    facet_wrap(~ variable, scales = "free"))+
  labs(caption="Figure 1.2 - Predictor variables visualization over entire period.")
```

We can see the impact of COVID across several of our predictors but, with no other obvious issues, it's time to move on to creating our predictive models.

### 3. The Approach

I tested two different machine learning methods. The first method used to analyze the dataset is an extension of the support vector machine algorithm, known as support vector regression (SVR). SVR was chosen over the more classic CART and Random Forest algorithms as it offers a smoother prediction when used for regression, features more capacity for model tuning due to its hyperparameters, and is found in many instances to give more accurate regression predictions.

Our first step is to set up the training/testing split in the data and to scale all the numeric values as the ranges of the variables in our dataset vary dramatically.
```{r}
## training/testing index----
training <- 1:128
test <- 129:158

## Create a new dataframe of scaled for all but the 'period' feature ----
data.scaled <- data.frame(period = data$period,scale(data[,2:9]))
```

Using the tune function from the e1071 package, a two-stage grid search method is used to discover the ideal hyperparameters to minimize the root mean squared error (RMSE) in the resulting SVR model. In the first instance, a wide range of values are specified for the hyperparameters kernel, cost and epsilon. 

```{r}
# Support vector regression model----
library(e1071)

## Create model----
system.time(tune.SVR <- tune(svm, unemploymentRate ~.,
                 data=data.scaled[training,-1],
                 ranges = list(epsilon = seq(0,1,0.1),
                               cost = 2^(2:9),
                               kernel = c("linear","radial","polynomial","sigmoid")))
)

print(tune.SVR)
```

The tune function returns 352 SVR models, with every possible combination of the specified hyperparameter values.

The tune function also returns the ideal hyperparameter values in order to minimize RMSE (epsilon = 0.4, cost = 16, kernel = radial). The minimal RMSE in this instance is 0.2180367.

In order to identify cluster areas of well-performing models for further examination, let's visualize the results.
```{r}
svr.perf <- tune.SVR$performances

## fig 2.1----
svr_cost_v_kernel <- ggplot(svr.perf,aes(x=kernel, y = as.factor(cost), colour = error))+
  geom_jitter(size=5,width=0.2,height=0.2)+
  scale_colour_gradient(trans="log10",
                        limits=c(min(svr.perf$error),1),
                        na.value = "red")+
  labs(x="kernel type",
       y="Cost setting",
       caption="Figure 2.1 - Grid search plot of cost vs kernel parameters. \n Jitter has been added to point positions for visibility.") +
    theme(legend.key.height = unit(2, "cm"))

svr_cost_v_kernel
```

Figure 2.1 shows the combinations of kernel and cost. Each point is jittered randomly for better visibility. It is clear that kernels 'polynomial' and 'sigmoid' (Red values indicate an RMSE of over 1) perform poorly on this dataset regardless of the cost parameter. Both are removed and the combination of cost and epsilon are examined. The linear kernel returns a very even error around 0.3. The radial kernel shows a higher range errors, both higher and lower than linear.

```{r}
## fig 2.2----
svr_cost_v_epsilon <- svr.perf %>% 
  filter(kernel %in% c("linear","radial")) %>% 
  ggplot(aes(x=epsilon, y = as.factor(cost), colour = error, shape = kernel))+
  geom_jitter(size=5,width=0.05, height=0.05)+
  scale_colour_gradient(trans="log10",
                        limits=c(min(svr.perf$error),0.4),
                        na.value = "red")+
  labs(y="Cost setting",
       x="Epsilon setting",
       caption="Figure 2.2 - Grid search plot of cost vs epsilon parameters across kernel types.")

svr_cost_v_epsilon
```
Figure 2.2 confirms that radial kernel gives a wider range of RMSE values including the lowest ones. 
```{r}
system.time(tune.SVR2 <- tune(svm, unemploymentRate ~.,
                              data=data.scaled[training,-1],
                              kernel="radial",
                              ranges = list(epsilon = seq(0,0.6,0.02),
                                            cost = seq(1,7,0.25)))
)

print(tune.SVR2)
```

The second tune returns 775 SVR models. Let's visualize the results to find the optimal parameters:

```{r}
plot(tune.SVR2)
```

The grid search visualization shows that the lowest RMSE (~0.22) is achieved around epsilon = 0.24 and cost = 3.5. These values, along with a radial kernel, will be our final hyperparameters for the SVR model.

Let's create our final model with these optimized parameters:

```{r}
# create final SVR model
system.time(model.svr <- svm(unemploymentRate ~.,
                 data = data.scaled[training,-1],
                 kernel='radial',
                 cost=3.5,
                 epsilon=0.24)
)

# assess performance against training data
train.pred.svr <- predict(model.svr, data.scaled[training,-1])
(train.rmse.svr <- sqrt(mean((data.scaled$unemploymentRate[training] - train.pred.svr)^2)))
```

For comparison, let's create a baseline model using the mean of our training data:

```{r}
baseline.pred <- rep(mean(data.scaled$unemploymentRate[training]), length(data.scaled$unemploymentRate))
(train.baseline <- sqrt(mean((data.scaled$unemploymentRate[training] - baseline.pred[training])^2)))
```

Our SVR model returns a training RMSE of approximately 0.27, compared to the baseline RMSE of about 1.0. This indicates that our model performs significantly better than simply predicting the mean unemployment rate.

Let's calculate the R-squared value to understand how much variance our model explains:

```{r}
train.svr.sse <- sum((data.scaled$unemploymentRate[training] - train.pred.svr)^2)
train.svr.sst <- sum((data.scaled$unemploymentRate[training] - mean(data.scaled$unemploymentRate[training]))^2)
(train.svr.r2 <- 1 - train.svr.sse/train.svr.sst)
```

Now let's evaluate the model's performance on our test data:

```{r}
test.pred.svr <- predict(model.svr, data.scaled[test,-1])
(test.rmse.svr <- sqrt(mean((data.scaled$unemploymentRate[test] - test.pred.svr)^2)))
(test.baseline <- sqrt(mean((data.scaled$unemploymentRate[test] - baseline.pred[test])^2)))

test.svr.sse <- sum((data.scaled$unemploymentRate[test] - test.pred.svr)^2)
test.svr.sst <- sum((data.scaled$unemploymentRate[test] - mean(data.scaled$unemploymentRate[test]))^2)
(test.svr.r2 <- 1 - test.svr.sse/test.svr.sst)
```

Let's visualize how our model performs across the entire dataset:

```{r}
svr.plot.df <- data.frame(
  actual = data.scaled$unemploymentRate,
  fitted = c(train.pred.svr, test.pred.svr),
  period = data.scaled$period
)

ggplot(svr.plot.df, aes(x=period, y=value)) +
  geom_line(aes(y=actual, color="Actual")) +
  geom_line(aes(y=fitted, color="Predicted")) +
  geom_vline(aes(xintercept=as.POSIXct("2012-03-01")), 
             color="black", linetype="dashed") +
  labs(title="SVR Model Performance",
       y="Standardized Unemployment Rate",
       color="Legend") +
  theme_minimal()
```
Considering the graph we can see the model gives accurate predictions for the training set but quickly departs from the actuals shortly after the cut off between training and testing sets. The testing set looks to give an almost inverse prediction. There is still a predicted uptick at the outset of the COVID pandemic though it's far less extreme than the actual value. This looks like our model is overfitting to the training data.


### Neural Network Implementation

For our second approach, we'll implement a neural network using the neuralnet package. Neural networks are particularly good at capturing complex, non-linear relationships in data, which makes them well-suited for economic predictions where many factors interact in subtle ways.

```{r}
library(neuralnet)

# Initial neural network model
set.seed(999)  # for reproducibility
nn1 <- neuralnet(unemploymentRate~.,
                 data=data.scaled[training,-1],
                 hidden=c(6,6,6),
                 linear.output=TRUE,
                 threshold=0.01)

# Evaluate on training data
train.pred.nn1 <- compute(nn1, data.scaled[training,-1])
(train.rmse.nn1 <- sqrt(mean((data.scaled$unemploymentRate[training] - train.pred.nn1$net.result)^2)))

# Evaluate on test data
test.pred.nn1 <- compute(nn1, data.scaled[test,-1])
(test.rmse.nn1 <- sqrt(mean((data.scaled$unemploymentRate[test] - test.pred.nn1$net.result)^2)))
```
Then we calculate the r-squared,
```{r}
test.nn1.sse <- sum((data.scaled$unemploymentRate[test] - test.pred.nn1[[2]])^2)
test.nn1.sst <- sum((data.scaled$unemploymentRate[test] - mean(data.scaled$unemploymentRate[test]))^2)
(test.nn1.r2 <- 1 - test.nn1.sse/test.nn1.sst)
```

To optimize our neural network architecture, we'll perform a grid search over different combinations of layers and neurons:

```{r}
# Grid search for optimal architecture
layers <- 1:5
neurons <- 1:8
nn.specs <- data.frame(layers=numeric(), neurons=numeric(), RMSE=numeric(), R2=numeric())

# Create formula explicitly
formula <- as.formula(paste("unemploymentRate ~", 
                          paste(names(data.scaled[,-c(1,2)]), collapse = " + ")))

for (l in layers) {
  for (n in neurons) {
    print(paste("Testing layers:", l, "neurons:", n))
    
    tryCatch({
      set.seed(999)
      # Train model with adjusted parameters for better convergence
      nn <- neuralnet(formula,
                      data=data.scaled[training,],
                      hidden=rep(n, l),
                      linear.output=TRUE,
                      threshold=0.1,  # Less strict threshold
                      stepmax=1e6,    # More iterations allowed
                      rep=1)          # Single repetition
      
      # Only proceed if model converged
      if(!is.null(nn$result.matrix)) {
        # Prepare test data
        test.matrix <- as.matrix(data.scaled[test, -c(1,2)])  # Remove period and unemployment
        
        # Make predictions
        pred.test <- predict(nn, data.scaled[test,])
        
        # Calculate metrics
        RMSE <- sqrt(mean((data.scaled$unemploymentRate[test] - pred.test)^2))
        
        test.nn.sse <- sum((data.scaled$unemploymentRate[test] - pred.test)^2)
        test.nn.sst <- sum((data.scaled$unemploymentRate[test] - mean(data.scaled$unemploymentRate[test]))^2)
        R2 <- 1 - test.nn.sse/test.nn.sst
        
        # Store results
        nn.specs <- rbind(nn.specs, data.frame(layers=l, neurons=n, RMSE=RMSE, R2=R2))
      } else {
        print(paste("Model did not converge for layers:", l, "neurons:", n))
        nn.specs <- rbind(nn.specs, data.frame(layers=l, neurons=n, RMSE=NA, R2=NA))
      }
    }, error = function(e) {
      print(paste("Error occurred with layers:", l, "neurons:", n))
      print(e)
      nn.specs <- rbind(nn.specs, data.frame(layers=l, neurons=n, RMSE=NA, R2=NA))
    })
  }
}

# Remove any rows with NA values for visualization
nn.specs_clean <- na.omit(nn.specs)

# Visualize results
ggplot(nn.specs_clean, aes(x=as.factor(layers), y=RMSE)) +
  geom_boxplot() +
  labs(x="Number of Hidden Layers",
       y="RMSE",
       title="Neural Network Performance by Architecture")
```

Based on our grid search results, we'll create our final neural network model with the optimal architecture two layers with 4 neurons each:

```{r}
# Final neural network model
set.seed(999)
nn2 <- neuralnet(formula,
                 data=data.scaled[training,-1],
                 hidden=c(4,4),      # Two hidden layers with 4 neurons each
                 linear.output=TRUE,
                 threshold=0.1,      # Less strict threshold (increased from 0.01)
                 stepmax=1e6,        # Increase maximum steps
                 learningrate=0.01)  # Add learning rate to help convergence

# Evaluate final model
train.pred.nn2 <- predict(nn2, data.scaled[training,-c(1,2)])
test.pred.nn2 <- predict(nn2, data.scaled[test,-c(1,2)])

# Calculate performance metrics
train.rmse.nn2 <- sqrt(mean((data.scaled$unemploymentRate[training] - train.pred.nn2)^2))
test.rmse.nn2 <- sqrt(mean((data.scaled$unemploymentRate[test] - test.pred.nn2)^2))

test.nn2.sse <- sum((data.scaled$unemploymentRate[test] - test.pred.nn2)^2)
test.nn2.sst <- sum((data.scaled$unemploymentRate[test] - mean(data.scaled$unemploymentRate[test]))^2)
test.nn2.r2 <- 1 - test.nn2.sse/test.nn2.sst

# Visualize final model performance
nn.plot.df <- data.frame(
  actual = data.scaled$unemploymentRate,
  fitted = c(train.pred.nn2, test.pred.nn2),
  period = data.scaled$period
)

ggplot(nn.plot.df, aes(x=period)) +
  geom_line(aes(y=actual, color="Actual")) +
  geom_line(aes(y=fitted, color="Predicted")) +
  geom_vline(xintercept=as.POSIXct("2018-03-01"), 
             linetype="dashed", color="black") +
  labs(title="Neural Network Model Performance",
       y="Standardized Unemployment Rate",
       color="Legend") +
  theme_minimal()
```
The graph shows that the neural network appears to be a large imrpovement over the SVR model. Let's compare the performance of the two.


### Model Comparison and Conclusions

```{r}
comparison_df <- data.frame(
  Model = c("SVR", "Neural Network"),
  Training_RMSE = c(train.rmse.svr, train.rmse.nn2),
  Test_RMSE = c(test.rmse.svr, test.rmse.nn2),
  Test_R2 = c(test.svr.r2, test.nn2.r2)
)

print(comparison_df)
```
Interestingly, while both models share a similar predictive power in the training set, the neural network shows an improved performance in the test set. This indicates that it has generalsed well in it's use of the predictor variables.

Both models demonstrated different strengths and weaknesses:

1. The SVR model:
   - Faster training time (0.08 seconds vs 2.14 seconds for NN)
   - More interpretable results
   - Terrible performance on test data
   - Predictied but underestimated the COVID-19 impact

2. The Neural Network model:
   - Better overall performance on both training and test data
   - Better at capturing non-linear relationships
   - Successfully predicted the general trend during COVID-19
   - Longer training time and less interpretable

### Future Improvements

Several potential improvements could enhance the models' performance:

1. Data Enhancements:
   - Include international economic indicators
   - Add government policy indicators
   - Incorporate leading indicators like business confidence surveys

2. Methodological Improvements:
   - Implement time series cross-validation
   - Explore ensemble methods combining both models
   - Add regularization to prevent overfitting

3. Feature Engineering:
   - Create lag variables for key indicators
   - Add interaction terms between related variables
   - Develop composite indicators

This project demonstrates the potential of machine learning in economic forecasting while highlighting the importance of careful model selection and evaluation. The superior performance of the neural network suggests that complex, non-linear relationships play a significant role in unemployment dynamics, particularly during unusual events like the COVID-19 pandemic.